# Linear-Regression-Scratch

ğŸ“Œ Project Overview

This project implements Linear Regression from scratch using Python and Gradient Descent â€” without relying on machine learning libraries like scikit-learn.

Dataset: (x, y) points (scatter plot).

Goal: Fit the best line 
ğ‘¦
=
ğ‘š
ğ‘¥
+
ğ‘
y=mx+b that minimizes error.

Method: Gradient Descent optimization.

Output: Fitted line plotted alongside data points.

âš™ï¸ How It Works

Initialize slope m and intercept b as 0.

Define a learning rate (L) and number of iterations (epochs).

Use gradient descent to iteratively update m and b.

After training, plot the best-fit line.

ğŸ§® Math Behind Linear Regression

We want to fit a line:

ğ‘¦
=
ğ‘š
ğ‘¥
+
ğ‘
y=mx+b
1. Error Function (MSE)

We use Mean Squared Error (MSE) to measure how good our line is:

MSE
(
ğ‘š
,
ğ‘
)
=
1
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
2
MSE(m,b)=
n
1
	â€‹

i=1
âˆ‘
n
	â€‹

(y
i
	â€‹

âˆ’(mx
i
	â€‹

+b))
2
2. Gradients

To minimize error, we compute derivatives:

For slope 
ğ‘š
m:

âˆ‚
âˆ‚
ğ‘š
MSE
(
ğ‘š
,
ğ‘
)
=
âˆ’
2
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
ğ‘¥
ğ‘–
â‹…
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
âˆ‚m
âˆ‚
	â€‹

MSE(m,b)=
n
âˆ’2
	â€‹

i=1
âˆ‘
n
	â€‹

x
i
	â€‹

â‹…(y
i
	â€‹

âˆ’(mx
i
	â€‹

+b))

For intercept 
ğ‘
b:

âˆ‚
âˆ‚
ğ‘
MSE
(
ğ‘š
,
ğ‘
)
=
âˆ’
2
ğ‘›
âˆ‘
ğ‘–
=
1
ğ‘›
(
ğ‘¦
ğ‘–
âˆ’
(
ğ‘š
ğ‘¥
ğ‘–
+
ğ‘
)
)
âˆ‚b
âˆ‚
	â€‹

MSE(m,b)=
n
âˆ’2
	â€‹

i=1
âˆ‘
n
	â€‹

(y
i
	â€‹

âˆ’(mx
i
	â€‹

+b))
3. Update Rules

Using gradient descent, update 
ğ‘š
m and 
ğ‘
b:

ğ‘š
â†
ğ‘š
âˆ’
ğ¿
â‹…
âˆ‚
âˆ‚
ğ‘š
MSE
mâ†mâˆ’Lâ‹…
âˆ‚m
âˆ‚
	â€‹

MSE
ğ‘
â†
ğ‘
âˆ’
ğ¿
â‹…
âˆ‚
âˆ‚
ğ‘
MSE
bâ†bâˆ’Lâ‹…
âˆ‚b
âˆ‚
	â€‹

MSE

Where L = learning rate.
